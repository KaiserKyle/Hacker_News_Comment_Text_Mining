{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541940, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_csv(\"d:\\\\data\\\\ISTFinal\\FinalProjectData.csv\")\n",
    "data = data.replace(np.nan, '', regex=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.3)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial, Stop words included, 1-gram\n",
      "0.6462580663190889\n",
      "Multinomial, Stop words excluded, 1-gram\n",
      "0.6408964037670681\n",
      "Multinomial, Stop words included, 1 and 2-grams\n",
      "0.6563691455575009\n",
      "Bernoulli, Stop words included, 1-gram\n",
      "0.6314854013940661\n",
      "Bernoulli, Stop words excluded, 1-gram\n",
      "0.6495027065145584\n",
      "Bernoulli, Stop words included, 1 and 2-gram\n",
      "0.6395982863645948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, data.CommentText, data.GoodComment, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(\"Multinomial, Stop words included, 1-gram\")\n",
    "print(avg)\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False, stop_words = 'english')),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, data.CommentText, data.GoodComment, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(\"Multinomial, Stop words excluded, 1-gram\")\n",
    "print(avg)\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False, ngram_range = (1,2))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, data.CommentText, data.GoodComment, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(\"Multinomial, Stop words included, 1 and 2-grams\")\n",
    "print(avg)\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=True)),('nb', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, data.CommentText, data.GoodComment, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(\"Bernoulli, Stop words included, 1-gram\")\n",
    "print(avg)\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=True, stop_words='english')),('nb', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, data.CommentText, data.GoodComment, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(\"Bernoulli, Stop words excluded, 1-gram\")\n",
    "print(avg)\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=True, ngram_range = (1,2))),('nb', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, data.CommentText, data.GoodComment, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(\"Bernoulli, Stop words included, 1 and 2-gram\")\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6449711530181693"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), stop_words='english')\n",
    "X_train_vec = gram12_count_vectorizer.fit_transform(train.CommentText)\n",
    "X_test_vec = gram12_count_vectorizer.transform(test.CommentText)\n",
    "\n",
    "nb_clf= MultinomialNB()\n",
    "nb_clf.fit(X_train_vec,train.GoodComment)\n",
    "\n",
    "nb_clf.score(X_test_vec,test.GoodComment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-16.9838\t00 000         \t\t-6.8840\tcom x2f        \n",
      "\t-16.9838\t00 000000      \t\t-6.8790\twant           \n",
      "\t-16.9838\t00 00110010    \t\t-6.8696\tcode           \n",
      "\t-16.9838\t00 00z         \t\t-6.8366\tve             \n",
      "\t-16.9838\t00 017437      \t\t-6.8215\treally         \n",
      "\t-16.9838\t00 027         \t\t-6.8083\tmake           \n",
      "\t-16.9838\t00 037139      \t\t-6.7983\thttp x2f       \n",
      "\t-16.9838\t00 05          \t\t-6.6930\tuse            \n",
      "\t-16.9838\t00 067151      \t\t-6.6356\tthink          \n",
      "\t-16.9838\t00 08          \t\t-6.6262\tgood           \n",
      "\t-16.9838\t00 0a          \t\t-6.6109\thttp www       \n",
      "\t-16.9838\t00 0b          \t\t-6.3941\tdon            \n",
      "\t-16.9838\t00 0d          \t\t-6.3816\ttime           \n",
      "\t-16.9838\t00 0e          \t\t-6.2883\twork           \n",
      "\t-16.9838\t00 117         \t\t-6.2716\tx2f x2f        \n",
      "\t-16.9838\t00 11_02       \t\t-6.2190\tquot           \n",
      "\t-16.9838\t00 142         \t\t-6.2144\tpeople         \n",
      "\t-16.9838\t00 18446744073709551615\t\t-6.2108\tjust           \n",
      "\t-16.9838\t00 18x24       \t\t-6.1493\tnofollow http  \n",
      "\t-16.9838\t00 19          \t\t-6.1459\thref http      \n",
      "\t-16.9838\t00 1975        \t\t-6.1455\twww            \n",
      "\t-16.9838\t00 1998        \t\t-6.0385\tlike           \n",
      "\t-16.9838\t00 1999        \t\t-5.9423\trel nofollow   \n",
      "\t-16.9838\t00 2000        \t\t-5.9421\tnofollow       \n",
      "\t-16.9838\t00 2001        \t\t-5.9419\trel            \n",
      "\t-16.9838\t00 2004        \t\t-5.9348\thref           \n",
      "\t-16.9838\t00 2013        \t\t-5.4393\thttp           \n",
      "\t-16.9838\t00 27          \t\t-5.3662\tcom            \n",
      "\t-16.9838\t00 27126g      \t\t-5.1818\tx27            \n",
      "\t-16.9838\t00 28          \t\t-4.8798\tx2f            \n"
     ]
    }
   ],
   "source": [
    "def show_most_and_least_informative_features(vectorizer, clf, class_idx=0, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[class_idx], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[-n:])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_and_least_informative_features(gram12_count_vectorizer, nb_clf, class_idx=0, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial, Stop words excluded, 1 and 2-grams\n",
      "0.6484022119814673\n",
      "Bernoulli, Stop words excluded, 1 and 2-gram\n",
      "0.637743473793385\n"
     ]
    }
   ],
   "source": [
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False, stop_words = 'english', ngram_range = (1,2))),('nb', MultinomialNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, data.CommentText, data.GoodComment, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(\"Multinomial, Stop words excluded, 1 and 2-grams\")\n",
    "print(avg)\n",
    "\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=True, stop_words = 'english', ngram_range = (1,2))),('nb', BernoulliNB())])\n",
    "scores = cross_val_score(nb_clf_pipe, data.CommentText, data.GoodComment, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(\"Bernoulli, Stop words excluded, 1 and 2-gram\")\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "food: -4.198165431694334\n",
      "restaurant: -4.250809165179756\n",
      "place: -5.15367687672177\n",
      "went: -5.37682042803598\n",
      "minutes: -6.76311478915587\n",
      "ordered: -5.559141984829934\n",
      "service: -5.15367687672177\n",
      "good: -4.971355319927816\n",
      "just: -6.4754327167040895\n",
      "experience: -5.46383180502561\n",
      "salad: -6.4754327167040895\n",
      "came: -7.168579897264035\n",
      "terrible: -7.86172707782398\n",
      "order: -6.76311478915587\n",
      "like: -5.46383180502561\n",
      "took: -7.86172707782398\n",
      "waitress: -6.4754327167040895\n",
      "waiter: -6.76311478915587\n",
      "wait: -6.76311478915587\n",
      "table: -6.76311478915587\n",
      "\n",
      "p\n",
      "food: -4.198165431694334\n",
      "restaurant: -4.250809165179756\n",
      "best: -4.494431247837506\n",
      "great: -4.770684624465664\n",
      "good: -4.971355319927816\n",
      "service: -5.15367687672177\n",
      "place: -5.15367687672177\n",
      "amazing: -5.15367687672177\n",
      "went: -5.37682042803598\n",
      "fresh: -5.37682042803598\n",
      "like: -5.46383180502561\n",
      "really: -5.46383180502561\n",
      "experience: -5.46383180502561\n",
      "friends: -5.559141984829934\n",
      "friendly: -5.559141984829934\n",
      "ordered: -5.559141984829934\n",
      "sauce: -5.559141984829934\n",
      "nice: -5.559141984829934\n",
      "delicious: -5.559141984829934\n",
      "pizza: -5.66450250048776\n"
     ]
    }
   ],
   "source": [
    "def show_top20(classifier, vectorizer, classNum):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    top20 = np.argsort(classifier.feature_log_prob_[classNum])[-20:]\n",
    "    top20 = np.flip(top20)\n",
    "    print(classifier.classes_[classNum])\n",
    "    for i in top20:\n",
    "        print(\"{}: {}\".format(feature_names[i], classifier.coef_[0][i]))\n",
    "\n",
    "\n",
    "show_top20(clf, vectorizer, 0)\n",
    "print(\"\")\n",
    "show_top20(clf, vectorizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
